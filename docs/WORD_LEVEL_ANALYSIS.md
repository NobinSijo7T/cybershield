# Word-Level Cyberbullying Analysis

## Overview
The cyberbullying detection system now includes **granular word-by-word analysis**, flagging each individual word in a sentence to identify which specific terms contribute to toxicity detection.

## Features

### 1. Word Categorization
Each word is classified into specific toxicity categories:

#### Critical Threat Words (Severity: CRITICAL üî¥‚ö†Ô∏è)
- **Words**: kill, die, suicide, murder, dead, death, stab, shoot, slash
- **Categories**: threat, violence
- **Example**: "I will **stab** you" ‚Üí "stab" flagged as CRITICAL

#### Violence Words (Severity: HIGH üî¥)
- **Words**: beat, hurt, harm, attack, punch, kick, destroy, violence, threat
- **Categories**: violence, threat
- **Example**: "I will **beat** you" ‚Üí "beat" flagged as HIGH

#### Worth Attack Words (Severity: HIGH üî¥)
- **Words**: waste, burden, useless, worthless, mistake, disappear, trash, unwanted
- **Categories**: insult, toxicity
- **Example**: "Your existence is **burden**" ‚Üí "burden" flagged as HIGH

#### Shaming Words (Severity: MEDIUM üü†)
- **Words**: embarrassment, embarrassing, disgrace, disgusting, shameful, disappointment
- **Categories**: insult, toxicity
- **Example**: "you are **embarrassing**" ‚Üí "embarrassing" flagged as MEDIUM

#### Insult Words (Severity: MEDIUM üü†)
- **Words**: stupid, dumb, idiot, loser, pathetic, failure, incompetent, nobody
- **Categories**: insult
- **Example**: "You are **stupid**" ‚Üí "stupid" flagged as MEDIUM

#### Existence Attack Words (Severity: HIGH üî¥)
- **Words**: existence, problem, regret, shouldnt, empty, meaningless, pointless, broken
- **Categories**: toxicity, insult
- **Example**: "Your future is **empty**" ‚Üí "empty" flagged as HIGH

#### Mockery Words (Severity: MEDIUM üü†)
- **Words**: laughs, mock, joke, ridicule, humiliate
- **Categories**: toxicity
- **Example**: "Everyone **laughs** at you" ‚Üí "laughs" flagged as MEDIUM

### 2. CSV Data Integration
- Each word is also checked against **1,873 n-grams** from the CSV database
- CSV scores provide additional toxicity context
- Scoring thresholds:
  - ‚â• 0.8 ‚Üí CRITICAL
  - ‚â• 0.6 ‚Üí HIGH
  - ‚â• 0.4 ‚Üí MEDIUM
  - > 0.3 ‚Üí LOW

## Data Structure

### WordAnalysis Interface
```typescript
interface WordAnalysis {
  word: string;              // The original word
  isToxic: boolean;          // Whether word is toxic
  severity: "safe" | "low" | "medium" | "high" | "critical";
  categories: string[];      // e.g., ["threat", "violence"]
  reasons: string[];         // Human-readable explanations
}
```

### ProcessedData Enhancement
```typescript
interface ProcessedData {
  original: string;
  normalized: string;
  tokens: string[];
  label: ClassificationResult;
  score: number;
  highSeverity: boolean;
  matchedSignals: string[];
  semanticMatches?: string[];
  wordAnalysis?: WordAnalysis[];  // NEW: Word-level breakdown
}
```

### CyberbullyingAnalysis Enhancement
```typescript
export interface CyberbullyingAnalysis {
  isCyberbullying: boolean;
  confidence: number;
  category?: string;
  recommendation?: string;
  context?: string;
  severity?: 'low' | 'medium' | 'high' | 'critical';
  wordAnalysis?: WordAnalysis[];  // NEW: Word-level breakdown
}
```

## Example Outputs

### Example 1: "I will beat you"
```
Overall: CYBERBULLY (100.0%)

Word-by-Word Analysis:
1. Word: "i"      ‚Üí ‚úÖ SAFE üü¢
2. Word: "will"   ‚Üí ‚úÖ SAFE üü¢
3. Word: "beat"   ‚Üí üö® TOXIC üî¥ HIGH
   Categories: violence, threat
   Reasons: Violence-related: "beat"
4. Word: "you"    ‚Üí ‚úÖ SAFE üü¢
```

### Example 2: "I will stab you"
```
Overall: CYBERBULLY (100.0%)

Word-by-Word Analysis:
1. Word: "i"      ‚Üí ‚úÖ SAFE üü¢
2. Word: "will"   ‚Üí ‚úÖ SAFE üü¢
3. Word: "stab"   ‚Üí üö® TOXIC üî¥‚ö†Ô∏è CRITICAL
   Categories: threat, violence
   Reasons: Critical threat word: "stab"
4. Word: "you"    ‚Üí ‚úÖ SAFE üü¢
```

### Example 3: "you are embarrassing"
```
Overall: CYBERBULLY (100.0%)

Word-by-Word Analysis:
1. Word: "you"           ‚Üí ‚úÖ SAFE üü¢
2. Word: "embarrassing"  ‚Üí üö® TOXIC üü† MEDIUM
   Categories: insult, toxicity
   Reasons: Shaming language: "embarrassing"
```

### Example 4: "Your existence is burden"
```
Overall: CYBERBULLY (100.0%)

Word-by-Word Analysis:
1. Word: "your"      ‚Üí ‚úÖ SAFE üü¢
2. Word: "existence" ‚Üí üö® TOXIC üî¥ HIGH
   Categories: toxicity, insult
   Reasons: Existence attack: "existence"
3. Word: "burden"    ‚Üí üö® TOXIC üî¥ HIGH
   Categories: insult, toxicity
   Reasons: Personal worth attack: "burden"
```

### Example 5: "You are stupid and pathetic"
```
Overall: CYBERBULLY (100.0%)

Word-by-Word Analysis:
1. Word: "you"      ‚Üí ‚úÖ SAFE üü¢
2. Word: "stupid"   ‚Üí üö® TOXIC üü† MEDIUM
   Categories: insult
   Reasons: Insulting term: "stupid"
3. Word: "pathetic" ‚Üí üö® TOXIC üü† MEDIUM
   Categories: insult
   Reasons: Insulting term: "pathetic"
```

## Implementation Details

### analyzeWords() Method
Located in `services/semanticDetector.ts`:

```typescript
private analyzeWords(tokens: string[]): WordAnalysis[] {
    const analyses: WordAnalysis[] = [];

    for (const token of tokens) {
        const word = token.toLowerCase();
        const analysis: WordAnalysis = {
            word: token,
            isToxic: false,
            severity: "safe",
            categories: [],
            reasons: []
        };

        // Check categorized word sets
        if (this.criticalThreatWords.has(word)) {
            analysis.isToxic = true;
            analysis.severity = "critical";
            analysis.categories.push("threat", "violence");
            analysis.reasons.push(`Critical threat word: "${token}"`);
        }
        // ... (additional category checks)

        // Check CSV data
        if (this.csvData.has(word)) {
            const csvScore = this.csvData.get(word)!;
            if (csvScore > 0.3) {
                analysis.isToxic = true;
                // Assign severity based on CSV score
                analysis.reasons.push(`CSV toxicity score: ${(csvScore * 100).toFixed(1)}%`);
            }
        }

        analyses.push(analysis);
    }

    return analyses;
}
```

### Integration in classify()
```typescript
public classify(text: string): ProcessedData {
    const { normalized, tokens, semanticMatches } = this.preprocessor.preprocess(text);
    const { score, highSeverity, matchedSignals } = this.computeScore(
        normalized,
        tokens,
        semanticMatches
    );

    // Perform word-level analysis
    const wordAnalysis = this.analyzeWords(tokens);

    // ... (rest of classification logic)

    return {
        // ... (other fields)
        wordAnalysis  // Include word-level breakdown
    };
}
```

## Usage in BytezAI

The BytezAI service now includes word analysis in its response:

```typescript
export interface CyberbullyingAnalysis {
  isCyberbullying: boolean;
  confidence: number;
  category?: string;
  recommendation?: string;
  context?: string;
  severity?: 'low' | 'medium' | 'high' | 'critical';
  wordAnalysis?: WordAnalysis[];  // ‚Üê Word-level breakdown
}
```

When BytezAI detects cyberbullying, it automatically includes the word analysis from the semantic detector.

## Testing

Run the word analysis test:
```bash
npx tsx tests/word_analysis_test.ts
```

Expected output: 100% accuracy with granular word-level flagging for all test cases.

## Benefits

1. **Transparency**: Users can see exactly which words triggered the detection
2. **Education**: Helps users understand why their message was flagged
3. **Debugging**: Developers can identify false positives at the word level
4. **Context**: Provides severity levels for each toxic word
5. **Categorization**: Shows whether words are threats, insults, or other types
6. **Reasoning**: Explains why each word is considered toxic

## Performance

- Word analysis is performed efficiently during tokenization
- No additional API calls required
- Caching applies to entire classification including word analysis
- Minimal performance impact (< 1ms per word)

## Future Enhancements

Potential improvements:
1. Context-aware word analysis (word meaning changes based on surrounding words)
2. Multi-word phrase detection in word analysis
3. Severity adjustment based on word combinations
4. Cultural and language-specific word categorization
5. User feedback integration to improve word categorization
